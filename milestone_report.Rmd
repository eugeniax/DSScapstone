---
title: "Milestone Report of Text Prediction Project"
author: "E"
date: "25 July 2015"
output: html_document
---

## Introduction

This report is part of the Coursera data science capstone project of predicting text. The following sections explain data loading and sampling, cleaning and text mining of the HC Copora.  This report presents a basic report of summary statistics about the dataset and the charasristics of the sample text. Last, the plan for creating a prediction algorithm and Shiny app is outlined.

## Data Pre-processing

The training dataset (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available.

### Loading datasets

```{r eval=FALSE}
zipURL <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")) 
    download.file(zipURL, destfile = "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
```

For this project, we will only use the 3 English datasets.

```{r}
blogs <- readLines("./final/en_US/en_US.blogs.txt", encoding="UTF-8", skipNul=TRUE)

file_news <- file("./final/en_US/en_US.news.txt", "rb")
news <- readLines(file_news, encoding="UTF-8", skipNul=TRUE)
close(file_news)
rm(file_news)

twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul=TRUE)
```

### Summary statistics

```{r}
## summarize training data
library(stringr)
size_blogs <- file.size("./final/en_US/en_US.blogs.txt")/1024^2
size_news <- file.size("./final/en_US/en_US.news.txt")/1024^2
size_twitter <- file.size("./final/en_US/en_US.twitter.txt")/1024^2
wdcount_blogs <- str_count(blogs, "\\S+")
wdcount_news <- str_count(news, "\\S+")
wdcount_twitter <- str_count(twitter, "\\S+")
lines_blogs <- length(blogs)
lines_news <- length(news)
lines_twitter <- length(twitter)

dataSummary <- data.frame(
    fileName = c("Blogs","News","Twitter"),
    fileSize = c(round(size_blogs, digits = 2), 
                 round(size_news,digits = 2), 
                 round(size_twitter, digits = 2)),
    lineCount = c(lines_blogs,lines_news,lines_twitter),
    wordCount = c(sum(wdcount_blogs),sum(wdcount_news),sum(wdcount_twitter)),
    maxWords = c(max(wdcount_blogs),max(wdcount_news),max(wdcount_twitter)),
    minWords = c(min(wdcount_blogs),min(wdcount_news),min(wdcount_twitter)),
    meanWords = c(mean(wdcount_blogs),mean(wdcount_news),mean(wdcount_twitter))
)
dataSummary
```

### Sampling and Cleaning

To make exploratory data analysis easier, 1% of the training data was randomly sampled to form a sample corpus. Later when builing real model, a higher percentage sample will be taken to ensure more coverage.

```{r}
## sampling
set.seed(123)
sampleBlogs <- sample(blogs, length(blogs)*0.01)
sampleNews <- sample(news, length(news)*0.01)
sampleTw <- sample(twitter, length(twitter)*0.01)
sampleText <- c(sampleBlogs,sampleNews,sampleTw)
```

After combiling the samples from 3 datasets, the final sample corpus has `r length(sampleText)` lines of text.

The follow cleaning steps were performed on the sample corpus.

- remove URLs, tweeter accounts
- lower all the words
- remove numbers
- remove punctuation
- remove profanity words source from http://www.bannedwordlist.com/

```{r}
## clean up
library(RWeka)
library(tm)
#profanity lib
profURL <- "http://www.bannedwordlist.com/lists/swearWords.csv"
download.file(profURL, destfile = "swearWords.csv")
profanity <- c(t(read.csv("swearWords.csv",header=F)))

sampleText <- gsub(" #\\S*","",sampleText) #remove hashtags 
sampleText <- gsub("(f|ht)(tp)(s?)(://)(\\S*)", "", sampleText) #remove URLs (http, https, ftp)
sampleText <- gsub(" @[^\\s]+","",sampleText) #remove twitter account 
sampleText <- iconv(sampleText, "latin1", "ASCII", sub=" ") #remove non-printable
# sampleText <- gsub("[^0-9A-Za-z///' ]", "", sampleText) #remove all non english / non numeric 
corpus<-Corpus(VectorSource(sampleText))
corpus<-tm_map(corpus, content_transformer(tolower))
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeNumbers)
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus, removeWords, profanity)
```

We will use a word cloud to visualize the sample corpus.
```{r}
library(wordcloud)
wordcloud(corpus, scale=c(5,0.5), max.words=100, 
          random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, 
          colors=brewer.pal(8, "Dark2"))
```

## Tokenization and Exploratory Analysis

### Build n-gram models

Next, the sample corpus is tokenized to build basic n-gram models. Given that the final product will have to be hosted on Shiny with limited resource, we chose to build 3 basic models - unigram, bigram and trigram. 

```{r}
library(tau)
sample_df <- data.frame(text=unlist(sapply(corpus, '[',"content")),stringsAsFactors=F)
tokenize_ngrams <- function(x, n=3) {
    return(textcnt(x,method="string",n=n,decreasing=TRUE))}
unigrams <- tokenize_ngrams(sample_df,n=1)
bigrams <- tokenize_ngrams(sample_df,n=2)
trigrams <- tokenize_ngrams(sample_df,n=3)
```

Several R packages were tested for extracting n-grams from the complete corpus:

- `NgramTokenizer` (package `RWeka`) 
- `textcnt` (package `tau`) 
- `ngram` (package `ngram`)

Relatively speaking, `textcnt` from R-package `tau` has the best performance and was our final pick.

### Frequency count on n-grams

Frequency tables were constructed for each of the n-gram models. 

```{r}
## construct frequency tables
freq_tb <- function(txtcnt){
    return(data.frame(word=rownames(as.data.frame(unclass(txtcnt))),
                      freq=unclass(txtcnt)))}
unigramFreq <- freq_tb(unigrams)
bigramFreq <- freq_tb(bigrams)
trigramFreq <- freq_tb(trigrams)
```

Below are the exploratory analysis on the most frequent unigrams, bigrams and trigrams in the sample text.

```{r echo=F}
## plot ngram distribution
library(ggplot2)
ggplot(head(unigramFreq,20), aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title="Most Frequent Unigrams in Sample Text")

ggplot(head(bigramFreq,20), aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title="Most Frequent Bigrams in Sample Text")

ggplot(head(trigramFreq,20), aes(x=reorder(word,freq), y=freq, fill=freq)) +
    geom_bar(stat="identity") +
    theme_bw() +
    coord_flip() +
    theme(axis.title.y = element_blank()) +
    labs(y="Frequency", title="Most Frequent Trigrams in Sample Text")
```

## Prediction strategies and plans for Shiny app

To make predictions, the n-gram model with a frequency table can be used based. A straightforward prediction algorithm is to start with the trigram model to find the most likely next word. If none is found, then the bigram model is used. If still no match, then the unigram model, i.e. the most possible word irregardly of context is suggested. The text entry from the use might need the same cleaning that was performed on training data to increase chances of match.

The Shiny app will have a simple user interface where the user can enter a string of English text. Then the prediction model will echo the text entered by the user and suggest top 5 most-likely words as the next word.