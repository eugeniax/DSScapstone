---
title: "Milestone Report of Text Prediction Project"
author: "E"
date: "25 July 2015"
output: html_document
---

## Introduction

This report is part of the Coursera data science capstone project of predicting text. The following sections explain data loading and sampling, cleaning and text mining of the HC Copora.  This report presents a basic report of summary statistics about the dataset and the charasristics of the sample text. Last, the plan for creating a prediction algorithm and Shiny app is outlined.

## Data processing

The training dataset (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available.

### Loading datasets

```{r}
zipURL <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")) 
    download.file(zipURL, destfile = "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
```

For this project, we will only use the 3 English datasets.

```{r}
blogs <- readLines("./final/en_US/en_US.blogs.txt", encoding="UTF-8", skipNul=TRUE)

file_news <- file("./final/en_US/en_US.news.txt", "rb")
news <- readLines(file_news, encoding="UTF-8", skipNul=TRUE)
close(file_news)
rm(file_news)

twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul=TRUE)
```

### Summary statistics

```{r}
## summarize training data
library(stringr)
size_blogs <- file.size("./final/en_US/en_US.blogs.txt")/1024^2
size_news <- file.size("./final/en_US/en_US.news.txt")/1024^2
size_twitter <- file.size("./final/en_US/en_US.twitter.txt")/1024^2
wdcount_blogs <- str_count(blogs, "\\S+")
wdcount_news <- str_count(news, "\\S+")
wdcount_twitter <- str_count(twitter, "\\S+")
lines_blogs <- length(blogs)
lines_news <- length(news)
lines_twitter <- length(twitter)

dataSummary <- data.frame(
    fileName = c("Blogs","News","Twitter"),
    fileSize = c(round(size_blogs, digits = 2), 
                 round(size_news,digits = 2), 
                 round(size_twitter, digits = 2)),
    lineCount = c(lines_blogs,lines_news,lines_twitter),
    wordCount = c(sum(wdcount_blogs),sum(wdcount_news),sum(wdcount_twitter)),
    maxWords = c(max(wdcount_blogs),max(wdcount_news),max(wdcount_twitter)),
    minWords = c(min(wdcount_blogs),min(wdcount_news),min(wdcount_twitter)),
    meanWords = c(mean(wdcount_blogs),mean(wdcount_news),mean(wdcount_twitter))
)
dataSummary
```

### Sampling and Cleaning

To make exploratory data analysis easier, 1% of the training data was randomly sampled to form a sample corpus. Later when builing real model, a higher percentage sample will be taken to ensure more coverage.

```{r}
## sampling
set.seed(123)
sampleBlogs <- sample(blogs, length(blogs)*0.01)
sampleNews <- sample(news, length(news)*0.01)
sampleTw <- sample(twitter, length(twitter)*0.01)
sampleText <- c(sampleBlogs,sampleNews,sampleTw)
```

After combiling the samples from 3 datasets, the final sample corpus has `r length(sampleText)` lines of text.

The follow cleaning steps were performed on the sample corpus.

- remove URLs, tweeter accounts
- lower all the words
- remove numbers
- remove punctuation
- remove profanity words source from http://www.bannedwordlist.com/

```{r}
## clean up
library(RWeka)
library(tm)
#profanity lib
profURL <- "http://www.bannedwordlist.com/lists/swearWords.csv"
download.file(profURL, destfile = "swearWords.csv")
profanity <- c(t(read.csv("swearWords.csv",header=F)))

sampleText <- gsub(" #\\S*","",sampleText) #remove hashtags 
sampleText <- gsub("(f|ht)(tp)(s?)(://)(\\S*)", "", sampleText) #remove URLs (http, https, ftp)
sampleText <- gsub(" @[^\\s]+","",sampleText) #remove twitter account 
sampleText <- iconv(sampleText, "latin1", "ASCII", sub=" ") #remove non-printable
# sampleText <- gsub("[^0-9A-Za-z///' ]", "", sampleText) #remove all non english / non numeric 
corpus<-Corpus(VectorSource(sampleText))
corpus<-tm_map(corpus, content_transformer(tolower))
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeNumbers)
corpus<-tm_map(corpus, stripWhitespace)
corpus<-tm_map(corpus, removeWords, profanity)
```

