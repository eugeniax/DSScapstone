Coursera/Swiftkey Capstone Predictive Text Model
========================================================
author: Eugenia
date: August 2015

The Challenge
========================================================

The objective of Coursera Data Science Capstone project is to build an Shiny app that predicts the next word based on user input, very similar to Swiftkey, a predictive keyboard on mobile phones. 

While text prediction is well established topic under natual language process, given the running environment and app usage, this project strives to achieve a few additional goals:

- lean - limited memory and processing resources on shiny.io server
- fast - users expect instant result as they type
- intuitive - easy to use on small mobile phone screens  
- accurate - trained on large text database


How the App Works
========================================================

The UI was designed to be as simple as possible to resemble a mobile phone interface. 

**Input panel on the left**

User simply type into a text box, then press 'Enter' on the keyboard or 'Submit' button

**Output panel on the right**

The user input is first echoed to assure the user that the app has received the correct input.

The predicted next word will be displayed in large blue font.
Upto 3 more words will be shown below as additional suggested words.


Behind the Scene - ngram frequency tables
========================================================

Text corpus was obtained from HC Corpora (www.corpora.heliohost.org), particularly, English news, blogs and twitter. Data cleaning involves lowering cases, removing punctuation, special/non-pritable charaters, whitespaces, numbers and profanity. Specific to twitter, accounts, hashtags and URLs are removed. After comparing prediction performance, stopwords are kept.

Next, 20% sampled corpus was tokenized ionto trigrams, bigrams and unigrams. They were subsequently ordered by decreasing frequency to build ngram frequency tables, which serve as frequency dictionaries for the prediction model.


Behind the Scene - prediction model
========================================================

A simple back-off model was built for the purpose of next-word prediction. Starting with trigram, if the last 2 words from user's input match the first 2 words in the trigram, then a list of 3rd words ordered by frequency is returned. The top item is the predicted next word and, if available, upto 3 more words are also displaced as suggested words. In the case of no match in trigrams, the model resolves to look for the last word from user in bigrams. If still no match, the model returns the most common unigram.

We had chosen this simple back-off algorithm for the following reasons:

- computationally inexpensive
- easy and straightforward to implement
- relatively decent accuracy